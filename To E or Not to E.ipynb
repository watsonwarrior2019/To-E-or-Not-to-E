{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# To E or Not to E\n## Machine Vision Challenge\n\nWelcome to the ASL Challenge! Here what we'll be doing is going through a real life example of how IBM machine learning is being leveraged\nto help improve the lives of those who are hearing-impaired.\n\nJohn Pace from Mark III Systems is working on building an app that can perform sign language letter recognition in real time. This would allow a person to\nunderstand exactly what someone with ASL is trying to say as it happens, with minimal delay. It will essentially allow two people to communicate\nwith each other in their own native way of speaking. Communication barriers are subsequently broken down and it will be as if their is no distinction\nbetween speaking and signing.\n\nThis Notebook represents a template for rapidly executing a machine vision proof of concept using IBM Watson Studio. In the early stages of any data science project there will be a need to quickly assess whether there is a model capable of performing the inferences that are desired. For example, if we are to develop a deep learning model that will perform real-time ASL translation then we need to first determine if we can classify images of the ASL letters and words. \n\nA data scientist must conduct experiments, going through an iterative process of hypothesis testing and exploratory data analysis to gauge the feasibility and potential of any given model. In this case, we are making the hypothesis that we can differentiate a few ASL letters with a moderate sample size of images using a particular algorithm. The model will classify images based on whether they display the signs for the letters \u2018E\u2019, \u2018S\u2019, or \u2018Y\u2019. If we are not able to obtain a better than random accuracy result from our model then we would consider the need to revisit the hypothesis and the chosen algorithms.    \n\n## Step 1: Install & Load Packages\n\nThe first step in running a Notebook is to install and load all of the packages needed to import and process data, manipulate images, and train and test a machine vision models. There are some standard ingredients such as NumPy and Pandas, Python packages for managing numerical arrays and data frames respectively. To apply machine vision though we have also included some popular deep learning packages such as Keras and TensorFlow.\n\n##### Directions\n1) Click into the cell below labeled \"#Install Packages\" and click the \"Run\" button in the Notebook toolbar.\n\n2) Run the next cell labeled \"#Load Packages\".\n\n##### Expected Output\n\nWhen the cells have completed running then a number will appear within the \"In [ ]:\" statement at the upper left of each cell. The first cell will not produce any output when completed, but the second cell will post a statement that says \"Using TensorFlow backend.\""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Install Packages\n\n! pip install ibm-cos-sdk -q\n! pip install keras==2.1.4 -q\n! pip install tensorflow==1.9 -q\n! pip install h5py -q # let us save the model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Load Packages\n\nimport zipfile\nfrom ibm_botocore.client import Config\nimport ibm_boto3\nimport pandas as pd\nimport numpy as np\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense,GlobalAveragePooling2D\nfrom keras.applications import MobileNet , ResNet50\nfrom keras.preprocessing import image\nfrom keras.applications.mobilenet import preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport shutil\nfrom keras.preprocessing import image\nimport tensorflow\nimport types\nfrom botocore.client import Config\nfrom numpy import loadtxt\nfrom keras.models import load_model\nimport pickle\n%matplotlib inline\nfrom tensorflow import ConfigProto\nfrom tensorflow import InteractiveSession\nimport os\nimport requests \n\nclass WatsonWarriors:\n    def __init__(self,token):\n        self.host = \"https://api.watsonwarriors.ai:8487\"\n        self.token = token;\n    def complete(self):\n        return requests.post(url = self.host + '/validate/0', json = { 'data': 1 }, headers = { 'Authorization': 'Bearer '+self.token });"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 2: Get the Data\n\nData science projects rely on large sets of clean and curated data for exploratory analysis, as well as model training and testing. In the context of a machine vision model we need data in the form of a large quantity of labelled images. \n\nThe next section of code will import a zip file containing images nested within four folders. Each folder is labeled with the classification of the images contained in that folder, and labels are \u2018E\u2019, \u2018S\u2019, \u2018Y\u2019, and \u2018Control\u2019. Staging the images in this way allows our code to infer labels from the folders which images belong to. Some additional cleanup is needed to remove some bad images at the end of the code chunk.\n\n\n##### Directions\n1) Run the cell labeled \"#Get Data\".\n\n##### Expected Output\n\nThere is no expected output for this step. When the cell has completed running then a number will appear within the \"In [ ]:\" statement at the upper left of the cell."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Get Data\n\n# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n    plt.tight_layout()\n    plt.show()\nBATCH_SIZE = 8\nIMG_SHAPE  = 224 # Our training data consists of images with width of 150 pixels and height of 150 pixels\nimg_width = 224\nimg_height = 224\nimage_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)\n\n\nos.getcwd()\n\n#For this step, we are simply passing our \"credentials\" to IBM cloud to allow us to download data from IBM Cloud Storage. You can sort of think of it as entering a\n#website password. Besides our username and password, we're also telling it directly the name of the files we want to use, where we want the files to go, and which files\n#are not useful for our purposes.\n\n\ncredentials_1 = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-245853a8-b4bb-4cae-9f38-88a5a14930d8',\n    'IBM_API_KEY_ID': 'J2qAH8aiRNHF97QXXzMpRvukNb7berjw3QK9IMlNBQ1y',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.ng.bluemix.net/oidc/token',\n    'BUCKET': 'wildfirepredictionibm-donotdelete-pr-rcabpk7sog3qdr',\n    'FILE': 'ASL_Sample_Images.zip'\n}\n\ncos = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='J2qAH8aiRNHF97QXXzMpRvukNb7berjw3QK9IMlNBQ1y',\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\ncos.download_file(Bucket=credentials_1['BUCKET'],Key='ASL_Sample_Images.zip',Filename='ASL_Sample_Images.zip')\n\nwith zipfile.ZipFile('/home/dsxuser/work/ASL_Sample_Images.zip', 'r') as zip_ref:\n    zip_ref.extractall('/home/dsxuser/work/Sample_Images/')\n\n! rm \"./Sample_Images/ASL Images/Y/Y_421.jpg\"\n! rm \"./Sample_Images/ASL Images/S/S_421.jpg\"\n! rm \"./Sample_Images/ASL Images/E/E_421.jpg\"\n! rm \"./Sample_Images/ASL Images/Controls/Controls_421.jpg\""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 3: Discover Classes and Sample Size\n\nBefore we dig further into machine learning, first we need to understand what we are trying to predict. Sometimes we are trying to predict future values of a number, other times we want to know what group, or class, data belongs to. It has already been discussed that we are working with ASL images pertaining to the letters \"E\", \"S\", and \"Y\".These are known as our \"classes\", or \"labels\" as we referred to them earlier. We know which \"classes\" our images belong to, but it is important to make sure the computer does as well. A machine learning model only knows as much as we tell it. For this step, we will be simply be loading our images from the folder, verifying our anticipated classes of \"E\", \"S\", and \"Y\" along with \"Controls\". While \"Controls\" appears as a class of images, they really are just images we will be providing to the model to act as a sort of contrast to our chosen letters of \"E\", \"S\", and \"Y\". This will further highlight the individual characteristics of each class, and our classification algorithms will take care of the rest and find what makes each set of images unique. With enough data, it will know what class a new image belongs to as we receive it. \n\nHow will the model know which images belong to which class? Simple. When we load our images, they will be assigned a class based on the name of the folder they are in. So we have a separate folder for E, one for Y, one for S, and the last for Controls.\n\n##### Directions\n1) Run the cell labeled \"#Discover Classes and Sample Size\".\n\n##### Expected Output\n\nObserve the three outputs: Image Classes, Class E Contents, and Augmented images (what do you notice?). What you see here is what our model will see. The first two consist of text, and the third will be images displayed below our \"Discover Classes and Sample Size\" cell.\n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "#Discover Classes and Sample Size\n\n#Here we are setting up our \"training set\" for machine learning. You can think of it as a very carefully curated dataset that contains attributes we specifically want the model\n#to pick up on. \n\n\ntrain_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n\ntrain_dir = './Sample_Images/ASL Images/'\n\ne_dir = './Sample_Images/ASL Images/E/'\n\n\n\ntrain_generator=train_datagen.flow_from_directory(train_dir,   # need to put in the path to the folder that has the training set in it....the folder names are the classes.\n                                                 target_size=(224,224),\n                                                 color_mode='rgb',\n                                                 batch_size=8,   \n#                                                   validation = validate_dir , # batch size may need to be changed if the memory gets full\n                                                 class_mode='categorical',\n                                                 shuffle=True)\n\ntrain_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE, \n                                               directory=train_dir, \n                                               shuffle=True, \n                                               target_size=(IMG_SHAPE,IMG_SHAPE))\naugmented_images = [train_data_gen[0][0][7] for i in range(5)]\n\n\n#Outputs\nprint(\"\")\nprint(\"\")\nprint(\"\")\nprint(\"Here we will output our folder names, show example contents of the E folder, and plot a few images with special type of augmentation applied. Can you guess what was done to the images?\")\nprint(\"\")\nprint(\"\")\nprint(\"-Classes\")\nprint(os.listdir(train_dir))\nprint(\"\")\nprint(\"-Class 'E'\")\nprint(\"\")\nprint('%.50s' % os.listdir(e_dir))\nprint(\"\")\nprint(\"\")\nprint(\"Notice anything weird about these photos?\")\nplotImages(augmented_images)\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 4: Augment Images\n\nIn the next section we will enhance the original dataset by rotating and stretching the images that were provided. This will provide additional examples for training a model and give the model a richer set of conditions in which to make inferences. This is a critical step in performing image classification. As data scientists, it is difficult to collect data representative of every single way an image can appear. Image augmentation reshapes and repositions images in a variety of ways beyond just rotating the image and expanding and contracting it. After all, we all take photos differently, have different cameras. Some cameras have better zoom than others, higher/lower resolution, and so on. It is vital that we can teach our classification model to already account for these scenarios. With even a limited dataset, we can augment what we have and turn one image into two (or 3, 4, 5...), and strenghten the accuracy of our model; without ever having to take additional images. We'll show you two separate examples of ways images can be augmented: rotation and zooming\n\n##### Directions\n1) Run the cell below labeled \"#Augment Images\".\n\n##### Expected Output\n\nOnce the cells have run, you can take a look at how we've changed them. You'll view one set of images pertaining to a 70% rotation, and another where they have been zoomed in 60%."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Augment Images\n\nimage_gen = ImageDataGenerator(rescale=1./255, rotation_range=70)\n\ntrain_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE, \n                                               directory=train_dir, \n                                               shuffle=True, \n                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n\naugmented_images = [train_data_gen[0][0][3] for i in range(5)]\n\nprint(\"70% Rotation\")\nprint(\"\")\nprint(\"\")\nplotImages(augmented_images)\n\n\nimage_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.6)\n\ntrain_data_gen = image_gen.flow_from_directory(batch_size=BATCH_SIZE, \n                                               directory=train_dir, \n                                               shuffle=True, \n                                               target_size=(IMG_SHAPE, IMG_SHAPE))\n\naugmented_images = [train_data_gen[0][0][0] for i in range(5)]\n\nprint(\"60% Zoom\")\nprint(\"\")\nprint(\"\")\nplotImages(augmented_images)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 5: Import Pre-Trained Model\n\nTime to finally execute a model! Here you'll be working with one that has already been trained. Machine learning models, especially those that work with images, can take quite a while to run. Pre-training a model is important as it allows you use one that has already been taught, or \"trained\", to work with the dataset in question. As new data is generated and uploaded, having a model already trained allows one to save time. What a data scientist needs to be careful of, though, is making sure the model they've chosen to reuse is optimized for peak performance. You would not want to reuse something that is incomplete or inaccurate. Algorithms are not perfect, but we can build them to be as close to perfect as possible.\n\nThe algorithm and model used here is called MobileNet. MobileNet is a convolutional neural network optimized for high-performance image recognition without requiring the immense computation power typically needed to perform accurate image classification. A neural network essentially is an algorithm modeled to recognize patterns and behavior in a way similar how humans do it. It utilizes a network of \"neurons\" that observe details of the data you provide, and determine what features make your dataset unique. It tries to identify these features and their weight on a model, and then assign numerical \"weights\" to each of the features. A neural network will continuously rewrite itself, under the provided parameters, until it finds what it believes to be the most ideal balance of weights across all features. \n\nExample:\n\nA neural network runs through an imageset and identifies features A, B, and C. The first run through, they are given weights of 0.3, 0.5, and 0.2 respectively and check if that accurately describes your images. It will then run again with a new set of weights, and compare that to the original.\n\nAttempt 1:\nA = 0.3\nB = 0.5\nC = 0.2\n\nAttempt 2:\nA = 0.1\nB = 0.3\nC = 0.6\n\n.\n.\n.\n.\n\n\n##### Directions\n1) Run the cell below labeled \"#Import Pre-Trained Model\".\n\n##### Expected Output\n\nBelow our \"Import Pre-Trained Model\" cell you will see a table containing columns labeled \"Layer\", \"Output Shape\", and \"Param #\". What is happening is that the program is checking which parameters of the model used apply to our current problem. These parameters could be thought of details that identify the behavior and characteristics of our imageset."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#Import Pre-Trained Model\n\n#### download model\ncos.download_file(Bucket=credentials_1['BUCKET'],Key='asl_mobile_netv2.h5',Filename='asl_mobile_netv2.h5')     # first model is called 'asl_mobile_net.h5'   the second is called 'asl_mobile_netv2.h5'\nfrom keras.utils.generic_utils import CustomObjectScope\n\nwith CustomObjectScope({'relu6': keras.applications.mobilenet.relu6,'DepthwiseConv2D': keras.applications.mobilenet.DepthwiseConv2D}):\n    model = load_model('asl_mobile_netv2.h5')\nmodel.summary()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 6: View and Test Model\n\nNow our model is loaded and ready to be tested. This will be done by taking the pre-trained model and feeding it a series of images. For each of the images, it will try to determine which class it belongs to. The idea is that it should already know what an \"E\" image looks like, and the same for S and Y. The moment the model views an image, it will make a determination on which letter it pertains to. In a perfect world, it could identify each image's class 100% of the time. In reality, though, there are grey areas and similarities between the image types. Our three image classes of E, S, and Y have traits that they share, and others that are unique to each. With enough data, one can attempt to identify all of these traits. In practice, however, a few images from \"E\" will be labeled as \"S\", some \"S\" images will be labeled as \"Y\".......\n\n#### Directions\n\n1) Run the cell below labeled \"#View and Test Model\".\n\n##### Expected Output\n\nAn image is displayed along with three pieces of information: 1. Its filename (which will indicate whether it is \"E\", \"S\", or \"Y\"), 2. Probability that each class applies to an image), and 3. Predicted Class\n\nThe filename, when compared to the predicted class, will tell us whether or not the model accurately identified which letter describes each image. Because the three letters have similarities to each other in terms of visual appearance, we receive scores, or probabilities, that a certain letter is true for an image. They are ordered Controls, E, S, and Y.\n\nA scenario for a \"E\" image may look as follows....\n\nClass Probabilities [Controls, E, S, Y]:\n[0.07, 0.008, 0.227, 0.687]\n\nPredicted Class:\n\n\"E\"\n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#View and Test Model\n\nsess = tensorflow.Session()\nlabels = {}\nlabels[\"label_names\"] = [\"Control\" , \"E\" , \"S\" , \"Y\"]\nimg_list = [\"./Sample_Images/ASL Images/Controls/Controls_3.jpg\"  , \"./Sample_Images/ASL Images/Y/Y_63.jpg\"  , \"./Sample_Images/ASL Images/E/E_21.jpg\" , \"./Sample_Images/ASL Images/S/S_3.jpg\"  ]\nimg_path = img_list[2]\n\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nplt.imshow(x/255.)\n\nimg = tensorflow.read_file(img_path)\nimg = tensorflow.image.decode_jpeg(img, channels=3)\nimg.set_shape([None, None, 3])\nimg = tensorflow.image.resize_images(img, (224, 224))\nimg = img.eval(session=sess) # convert to numpy array\nimg = np.expand_dims(img, 0) # make 'batch' of 1\nprint(\"---Filename\")\nprint(\"\")\nprint(\"\")\nprint(img_path)\nprint(\"\")\nprint(\"\")\npred = model.predict(img)\nprint(\"---Class Probabilities, Predicted Class, Original Image\")\nprint(\"\")\nprint(\"\")\nprint(pred)\npred = labels[\"label_names\"][np.argmax(pred)]\npred\n\n#print(img_list[1])\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 7: Complete Challenge\n\n### Directions\n\n1) Go back to the Watson Warriors tab, and paste the code from the Finish the Challenge task below.\n\n2) Run the cell below.\n\n3) Return to the Watson Warriors tab to see your score!"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "## Paste code"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## This completes the challenge!!"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}